{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "916nCTAyN7l8"
      },
      "outputs": [],
      "source": [
        "import pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "lmb5ETSZQeVQ",
        "outputId": "c4e2e504-e274-430f-dc50-beb76c5363c9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Using incubator modules: jdk.incubator.vector\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/06/20 16:37:03 WARN Utils: Your hostname, Xiaochens-MacBook-Pro.local, resolves to a loopback address: 127.0.0.1; using 192.168.0.111 instead (on interface en0)\n",
            "25/06/20 16:37:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/06/20 16:37:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        },
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.UnsupportedOperationException: getSubject is supported only if a security manager is allowed\n\tat java.base/javax.security.auth.Subject.getSubject(Subject.java:347)\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:588)\n\tat org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2446)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2446)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:339)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:501)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:485)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1575)\n",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#create spark session\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m spark=\u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPractice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m spark\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/python_env_xiaochen/lib/python3.12/site-packages/pyspark/sql/session.py:556\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    554\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    559\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/python_env_xiaochen/lib/python3.12/site-packages/pyspark/core/context.py:523\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/python_env_xiaochen/lib/python3.12/site-packages/pyspark/core/context.py:207\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    205\u001b[39m SparkContext._ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway=gateway, conf=conf)\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m.stop()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/python_env_xiaochen/lib/python3.12/site-packages/pyspark/core/context.py:300\u001b[39m, in \u001b[36mSparkContext._do_init\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    297\u001b[39m \u001b[38;5;28mself\u001b[39m.environment[\u001b[33m\"\u001b[39m\u001b[33mPYTHONHASHSEED\u001b[39m\u001b[33m\"\u001b[39m] = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mPYTHONHASHSEED\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m \u001b[38;5;28mself\u001b[39m._jsc = jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conf\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[32m    302\u001b[39m \u001b[38;5;28mself\u001b[39m._conf = SparkConf(_jconf=\u001b[38;5;28mself\u001b[39m._jsc.sc().conf())\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/python_env_xiaochen/lib/python3.12/site-packages/pyspark/core/context.py:429\u001b[39m, in \u001b[36mSparkContext._initialize_context\u001b[39m\u001b[34m(self, jconf)\u001b[39m\n\u001b[32m    425\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    426\u001b[39m \u001b[33;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[32m    427\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/python_env_xiaochen/lib/python3.12/site-packages/py4j/java_gateway.py:1627\u001b[39m, in \u001b[36mJavaClass.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1621\u001b[39m command = proto.CONSTRUCTOR_COMMAND_NAME +\\\n\u001b[32m   1622\u001b[39m     \u001b[38;5;28mself\u001b[39m._command_header +\\\n\u001b[32m   1623\u001b[39m     args_command +\\\n\u001b[32m   1624\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1626\u001b[39m answer = \u001b[38;5;28mself\u001b[39m._gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1627\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1628\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1630\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1631\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/python_env_xiaochen/lib/python3.12/site-packages/py4j/protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
            "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.UnsupportedOperationException: getSubject is supported only if a security manager is allowed\n\tat java.base/javax.security.auth.Subject.getSubject(Subject.java:347)\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:588)\n\tat org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2446)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2446)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:339)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:501)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:485)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1575)\n"
          ]
        }
      ],
      "source": [
        "#create spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName(\"Practice\").getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LR5fqwyPQgpQ",
        "outputId": "e522a228-add9-42f3-807e-305d6c588b76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+----------+----+----------+------+\n",
            "|Name|Department| Age|experience|salary|\n",
            "+----+----------+----+----------+------+\n",
            "|  AA|        D1|  30|         2|   100|\n",
            "|  BB|        D1|  35|         1|   125|\n",
            "|  CC|        D3|  25|         3|   100|\n",
            "|  DD|        D2|  20|      NULL|    60|\n",
            "|  EE|      NULL|  29|         4|    95|\n",
            "|  FF|        D2|  22|         4|    90|\n",
            "|  GG|        D7|NULL|         1|    82|\n",
            "|  HH|        D2|  38|         6|   115|\n",
            "+----+----------+----+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# read data\n",
        "spark_df=spark.read.csv('test_file.csv',header=True,enforceSchema=True)\n",
        "spark_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPx4KgDWS4mj"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRRIlTp8Qjkf",
        "outputId": "666137b1-afe0-4267-d723-ece247892495"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Name', 'Department', 'Age', 'experience', 'salary']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#list of columns\n",
        "spark_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzVSgkUPQj2_",
        "outputId": "168eb65c-bfee-4e40-f7f5-22e020a32465"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Department: string (nullable = true)\n",
            " |-- Age: string (nullable = true)\n",
            " |-- experience: string (nullable = true)\n",
            " |-- salary: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhQnJ9R5QkJn",
        "outputId": "9008085d-34ab-4ad9-adbb-40be48184e12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Department: string (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- experience: integer (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Cast Columns to the Correct Data Types:\n",
        "\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "cols=[\"Age\",\"experience\",\"salary\"]\n",
        "for col in cols:\n",
        "  spark_df = spark_df.withColumn(col, spark_df[col].cast(IntegerType()))\n",
        "spark_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxey6wj5Qkd_",
        "outputId": "0e593943-f900-417f-9f53-6e81a540776e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+----------+----+----------+------+\n",
            "|Name|Department| Age|experience|salary|\n",
            "+----+----------+----+----------+------+\n",
            "|  AA|        D1|  30|         2|   100|\n",
            "|  BB|        D1|  35|         1|   125|\n",
            "|  CC|        D3|  25|         3|   100|\n",
            "|  DD|        D2|  20|      NULL|    60|\n",
            "|  EE|      NULL|  29|         4|    95|\n",
            "|  FF|        D2|  22|         4|    90|\n",
            "|  GG|        D7|NULL|         1|    82|\n",
            "|  HH|        D2|  38|         6|   115|\n",
            "+----+----------+----+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-aZ-FJ1QktX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnTdkzI-rqlM",
        "outputId": "8faf68fe-33cf-4bdc-e64f-bc475d330040"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+----------+----+\n",
            "|Name|Department| Age|\n",
            "+----+----------+----+\n",
            "|  AA|        D1|  30|\n",
            "|  BB|        D1|  35|\n",
            "|  CC|        D3|  25|\n",
            "|  DD|        D2|  20|\n",
            "|  EE|      NULL|  29|\n",
            "|  FF|        D2|  22|\n",
            "|  GG|        D7|NULL|\n",
            "|  HH|        D2|  38|\n",
            "+----+----------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#select columns\n",
        "sub_df=spark_df.select(['Name','Department', 'Age'])\n",
        "sub_df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdfgteUhWkJJ",
        "outputId": "f7d167a6-e22d-4e50-bf82-74126d5ae033"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8 5\n"
          ]
        }
      ],
      "source": [
        "#Check the size of data\n",
        "\n",
        "print(spark_df.count(),len(spark_df.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHKIi2frWx-0",
        "outputId": "3d9b748a-815a-4089-871b-9dbaccbe01d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Name', 'string'),\n",
              " ('Department', 'string'),\n",
              " ('Age', 'int'),\n",
              " ('experience', 'int'),\n",
              " ('salary', 'int')]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#check data types\n",
        "spark_df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gjwl82uIXEAp",
        "outputId": "dbe9c4aa-ef06-4859-d8ee-f6b9218dc7ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+----+----------+------------------+------------------+------------------+\n",
            "|summary|Name|Department|               Age|        experience|            salary|\n",
            "+-------+----+----------+------------------+------------------+------------------+\n",
            "|  count|   8|         7|                 7|                 7|                 8|\n",
            "|   mean|NULL|      NULL|28.428571428571427|               3.0|            95.875|\n",
            "| stddev|NULL|      NULL| 6.604471789556498|1.8257418583505538|19.866968278311326|\n",
            "|    min|  AA|        D1|                20|                 1|                60|\n",
            "|    max|  HH|        D7|                38|                 6|               125|\n",
            "+-------+----+----------+------------------+------------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Describe dataframe\n",
        "spark_df.describe().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCXSkdH5ZT8U",
        "outputId": "e1acd2fe-5c2a-4adc-e453-a375551d1926"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+----------+----+----------+------+------------------------+\n",
            "|Name|Department| Age|experience|salary|experience_after_2_years|\n",
            "+----+----------+----+----------+------+------------------------+\n",
            "|  AA|        D1|  30|         2|   100|                       4|\n",
            "|  BB|        D1|  35|         1|   125|                       3|\n",
            "|  CC|        D3|  25|         3|   100|                       5|\n",
            "|  DD|        D2|  20|      NULL|    60|                    NULL|\n",
            "|  EE|      NULL|  29|         4|    95|                       6|\n",
            "|  FF|        D2|  22|         4|    90|                       6|\n",
            "|  GG|        D7|NULL|         1|    82|                       3|\n",
            "|  HH|        D2|  38|         6|   115|                       8|\n",
            "+----+----------+----+----------+------+------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Adding new column\n",
        "spark_df=spark_df.withColumn('experience_after_2_years',spark_df['experience']+2)\n",
        "spark_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMzCqHoTaKEN",
        "outputId": "0b0dea75-65a8-4c3e-c2dd-e89f060c186a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+----------+----+----------+------+\n",
            "|Name|Department| Age|experience|salary|\n",
            "+----+----------+----+----------+------+\n",
            "|  AA|        D1|  30|         2|   100|\n",
            "|  BB|        D1|  35|         1|   125|\n",
            "|  CC|        D3|  25|         3|   100|\n",
            "|  DD|        D2|  20|      NULL|    60|\n",
            "|  EE|      NULL|  29|         4|    95|\n",
            "|  FF|        D2|  22|         4|    90|\n",
            "|  GG|        D7|NULL|         1|    82|\n",
            "|  HH|        D2|  38|         6|   115|\n",
            "+----+----------+----+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#drop column(s)\n",
        "spark_df=spark_df.drop('experience_after_2_years')\n",
        "spark_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5chnq2KLbEAG",
        "outputId": "a7d2e795-8a98-4c7a-ce51-1c2a69e716ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+----------+----+----------+------+\n",
            "|Full Name|Department| Age|experience|salary|\n",
            "+---------+----------+----+----------+------+\n",
            "|       AA|        D1|  30|         2|   100|\n",
            "|       BB|        D1|  35|         1|   125|\n",
            "|       CC|        D3|  25|         3|   100|\n",
            "|       DD|        D2|  20|      NULL|    60|\n",
            "|       EE|      NULL|  29|         4|    95|\n",
            "|       FF|        D2|  22|         4|    90|\n",
            "|       GG|        D7|NULL|         1|    82|\n",
            "|       HH|        D2|  38|         6|   115|\n",
            "+---------+----------+----+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#rename column(s)\n",
        "spark_df.withColumnRenamed('Name','Full Name').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVSOePcMbg4n",
        "outputId": "79b7b291-5670-423a-f946-015fd8573e48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+------+----+----------+------+\n",
            "|Full Name|Sector| Age|experience|salary|\n",
            "+---------+------+----+----------+------+\n",
            "|       AA|    D1|  30|         2|   100|\n",
            "|       BB|    D1|  35|         1|   125|\n",
            "|       CC|    D3|  25|         3|   100|\n",
            "|       DD|    D2|  20|      NULL|    60|\n",
            "|       EE|  NULL|  29|         4|    95|\n",
            "|       FF|    D2|  22|         4|    90|\n",
            "|       GG|    D7|NULL|         1|    82|\n",
            "|       HH|    D2|  38|         6|   115|\n",
            "+---------+------+----+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#rename column(s)\n",
        "#new_df=spark_df.withColumnRenamed('Name','Full Name').withColumnRenamed('Department','Sector')\n",
        "\n",
        "new_df=spark_df.withColumnsRenamed({'Name':'Full Name','Department':'Sector' })\n",
        "new_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQlDWZRmzyfm",
        "outputId": "b3156687-c0b9-472a-e31a-d8b449440d44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+----------+----+----------+----+\n",
            "|Name|Department| Age|experience|wage|\n",
            "+----+----------+----+----------+----+\n",
            "|  AA|        D1|  30|         2| 100|\n",
            "|  BB|        D1|  35|         1| 125|\n",
            "|  CC|        D3|  25|         3| 100|\n",
            "|  DD|        D2|  20|      NULL|  60|\n",
            "|  EE|      NULL|  29|         4|  95|\n",
            "|  FF|        D2|  22|         4|  90|\n",
            "|  GG|        D7|NULL|         1|  82|\n",
            "|  HH|        D2|  38|         6| 115|\n",
            "+----+----------+----+----------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Rename columns\n",
        "spark_df.withColumnRenamed('salary','wage').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d-Phz0ogwB7"
      },
      "outputs": [],
      "source": [
        "#Stop session\n",
        "\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nx-nuYcLdK05"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "python_env_xiaochen",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
